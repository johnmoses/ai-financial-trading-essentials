{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Production Deployment and Infrastructure at Scale\n",
    "\n",
    "## 1. Low-Latency Architecture: Network Optimization, Hardware Acceleration, FPGA/GPU Usage\n",
    "\n",
    "In many trading strategies, especially high-frequency trading (HFT), latency is a critical factor. Even a millisecond delay can make the difference between a profitable and a losing trade.\n",
    "\n",
    "- Network Optimization: This involves minimizing the time it takes for data to travel between the trading system and the exchange. This is often achieved by co-locating servers in the same data center as the exchange's servers.\n",
    "- Hardware Acceleration: Specialized hardware like FPGAs (Field-Programmable Gate Arrays) and GPUs (Graphics Processing Units) can be used to accelerate complex calculations, such as those involved in pricing complex derivatives or running machine learning models.\n",
    "- FPGA/GPU Usage: FPGAs are highly customizable and can be programmed to perform specific tasks with very low latency. GPUs are well-suited for parallel processing and are often used for training and running deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c065d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Low-Latency Calculation (Conceptual) ---\n",
      "NumPy (CPU) took: 0.0170 seconds\n",
      "CuPy (GPU) calculation is conceptual and requires a CUDA-enabled GPU.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import cupy as cp  # You would need to have cupy installed and a CUDA-enabled GPU\n",
    "import time\n",
    "\n",
    "# Using NumPy (CPU)\n",
    "def numpy_calculation(size=10000):\n",
    "    a = np.random.rand(size, size)\n",
    "    b = np.random.rand(size, size)\n",
    "    start_time = time.time()\n",
    "    c = np.dot(a, b)\n",
    "    end_time = time.time()\n",
    "    print(f\"NumPy (CPU) took: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "# Conceptual example with CuPy (GPU)\n",
    "def cupy_calculation(size=10000):\n",
    "    # a = cp.random.rand(size, size)\n",
    "    # b = cp.random.rand(size, size)\n",
    "    # start_time = time.time()\n",
    "    # c = cp.dot(a, b)\n",
    "    # end_time = time.time()\n",
    "    # print(f\"CuPy (GPU) took: {end_time - start_time:.4f} seconds\")\n",
    "    print(\"CuPy (GPU) calculation is conceptual and requires a CUDA-enabled GPU.\")\n",
    "\n",
    "\n",
    "print(\"--- Low-Latency Calculation (Conceptual) ---\")\n",
    "numpy_calculation(1000)\n",
    "cupy_calculation(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Serving and Real-Time Inference: REST APIs, gRPC, Message Queues\n",
    "\n",
    "Once a model is trained, it needs to be deployed in a way that it can be used for real-time inference (i.e., making predictions on new data).\n",
    "\n",
    "- REST APIs: Representational State Transfer (REST) APIs are a popular choice for model serving. They are easy to implement and use, but they can have higher latency compared to other options.\n",
    "- gRPC: gRPC is a high-performance, open-source RPC (Remote Procedure Call) framework developed by Google. It uses HTTP/2 for transport and Protocol Buffers as the interface description language, which makes it more efficient than REST.\n",
    "- Message Queues: For very low-latency applications, message queues like RabbitMQ or ZeroMQ can be used. The model can listen for incoming data on a message queue, process it, and then publish the prediction to another queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d897cfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FastAPI Model Serving (Conceptual) ---\n",
      "The code above defines a simple FastAPI application for model serving.\n",
      "To run it, you would need to have fastapi and uvicorn installed.\n"
     ]
    }
   ],
   "source": [
    "# You would need to install fastapi and uvicorn first\n",
    "# pip install fastapi uvicorn\n",
    "from fastapi import FastAPI\n",
    "# import uvicorn # For running the app\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load your trained model here\n",
    "# model = load_model(\"my_trading_model.pkl\")\n",
    "\n",
    "async def predict(data: dict):\n",
    "    \"\"\"A simple model serving endpoint.\"\"\"\n",
    "    # In a real application, you would preprocess the data and then\n",
    "    # use your model to make a prediction.\n",
    "    # prediction = model.predict(data)\n",
    "    prediction = \"buy\" # Mock prediction\n",
    "    return {\"prediction\": prediction}\n",
    "\n",
    "# To run this app, you would save it as a Python file (e.g., main.py) and then run:\n",
    "# uvicorn main:app --reload\n",
    "\n",
    "print(\"--- FastAPI Model Serving (Conceptual) ---\")\n",
    "print(\"The code above defines a simple FastAPI application for model serving.\")\n",
    "print(\"To run it, you would need to have fastapi and uvicorn installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automated Model Retraining and Deployment Pipelines\n",
    "\n",
    "Financial markets are constantly evolving, which means that trading models can become stale over time. To address this, it is essential to have automated pipelines for retraining and deploying models.\n",
    "\n",
    "- Continuous Retraining: This involves automatically retraining the models on new data on a regular basis (e.g., daily or weekly).\n",
    "- Seamless Deployment: The new models should be deployed seamlessly without disrupting live trading. This can be achieved using techniques like blue-green deployment or canary releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e823924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You would need to install prefect first\n",
    "# pip install prefect\n",
    "from prefect import task, flow\n",
    "from prefect.schedulers import CronSchedule\n",
    "\n",
    "def fetch_new_data():\n",
    "    \"\"\"A mock task to fetch new data.\"\"\"\n",
    "    print(\"Fetching new data...\")\n",
    "    return \"new_data\"\n",
    "\n",
    "def retrain_model(data):\n",
    "    \"\"\"A mock task to retrain a model.\"\"\"\n",
    "    print(f\"Retraining model with {data}...\")\n",
    "    return \"new_model\"\n",
    "\n",
    "def deploy_model(model):\n",
    "    \"\"\"A mock task to deploy a model.\"\"\"\n",
    "    print(f\"Deploying {model}...\")\n",
    "\n",
    "# Run every day at midnight\n",
    "def retraining_pipeline():\n",
    "    \"\"\"A simple retraining pipeline.\"\"\"\n",
    "    new_data = fetch_new_data()\n",
    "    new_model = retrain_model(new_data)\n",
    "    deploy_model(new_model)\n",
    "\n",
    "# To run this scheduled flow, you would need to have a Prefect server running.\n",
    "# retraining_pipeline()\n",
    "\n",
    "print(\"--- Prefect Retraining Pipeline (Conceptual) ---\")\n",
    "print(\"The code above defines a scheduled retraining pipeline using Prefect.\")\n",
    "print(\"To run it as a scheduled flow, you would need a Prefect server.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Cloud and Edge Deployment Strategies\n",
    "\n",
    "Deploying a trading system across multiple cloud providers or at the edge (i.e., closer to the data source) can provide several benefits.\n",
    "\n",
    "- Multi-Cloud: Using multiple cloud providers can improve availability and resilience. If one cloud provider has an outage, the system can failover to another provider.\n",
    "- Edge Deployment: Deploying models at the edge can reduce latency by processing data closer to where it is generated. For example, a model could be deployed in a data center that is physically close to the exchange."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Context Protocol (MCP) Integration for Enhanced AI Coordination\n",
    "\n",
    "Model Context Protocol (MCP) is a protocol that facilitates coordination and context sharing between different AI agents in a trading system.\n",
    "\n",
    "Why it's useful: In a multi-agent system, it is important for the agents to be able to share information and context with each other. For example, a news analysis agent could share its sentiment analysis with a trading agent, which could then use that information to make a trading decision.\n",
    "\n",
    "Enhanced Intelligence: By sharing context, the agents can work together more effectively and make more intelligent decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb75b7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'protocol': 'MCP',\n",
       " 'version': '1.0',\n",
       " 'timestamp': '2025-09-27T10:30:00Z',\n",
       " 'source_agent': 'NewsAnalysisAgent',\n",
       " 'target_agent': 'TradingAgent',\n",
       " 'context': {'asset': 'AAPL',\n",
       "  'sentiment': 'positive',\n",
       "  'confidence': 0.85,\n",
       "  'summary': \"Apple's new iPhone sales exceed expectations, driving stock prices up.\"}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"protocol\": \"MCP\",\n",
    "  \"version\": \"1.0\",\n",
    "  \"timestamp\": \"2025-09-27T10:30:00Z\",\n",
    "  \"source_agent\": \"NewsAnalysisAgent\",\n",
    "  \"target_agent\": \"TradingAgent\",\n",
    "  \"context\": {\n",
    "    \"asset\": \"AAPL\",\n",
    "    \"sentiment\": \"positive\",\n",
    "    \"confidence\": 0.85,\n",
    "    \"summary\": \"Apple's new iPhone sales exceed expectations, driving stock prices up.\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Security Architecture: API Security, Data Encryption, Access Controls\n",
    "\n",
    "Security is a critical concern for any trading system. A security breach could result in significant financial losses.\n",
    "\n",
    "- API Security: APIs should be secured using techniques like API keys, OAuth, and rate limiting.\n",
    "- Data Encryption: Data should be encrypted both at rest (i.e., when it is stored on disk) and in transit (i.e., when it is being transmitted over the network).\n",
    "- Access Controls: Strict access controls should be in place to ensure that only authorized users have access to the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f61b1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FastAPI API Key Security (Conceptual) ---\n",
      "The code above shows how to secure a FastAPI endpoint with an API key.\n",
      "To access the /secure-data endpoint, you would need to include the X-API-Key header in your request.\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, Depends, HTTPException, status\n",
    "from fastapi.security import APIKeyHeader\n",
    "\n",
    "API_KEY = \"my-secret-api-key\"\n",
    "api_key_header = APIKeyHeader(name=\"X-API-Key\")\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "async def get_api_key(api_key: str = Depends(api_key_header)):\n",
    "    if api_key != API_KEY:\n",
    "        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Invalid API Key\")\n",
    "    return api_key\n",
    "\n",
    "async def get_secure_data(api_key: str = Depends(get_api_key)):\n",
    "    return {\"data\": \"This is secure data.\"}\n",
    "\n",
    "# To run this app, you would save it as a Python file (e.g., main.py) and then run:\n",
    "# uvicorn main:app --reload\n",
    "\n",
    "print(\"--- FastAPI API Key Security (Conceptual) ---\")\n",
    "print(\"The code above shows how to secure a FastAPI endpoint with an API key.\")\n",
    "print(\"To access the /secure-data endpoint, you would need to include the X-API-Key header in your request.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Disaster Recovery and Business Continuity Planning\n",
    "\n",
    "Disaster recovery and business continuity planning are essential for ensuring that the trading system can continue to operate in the event of a failure or disaster.\n",
    "\n",
    "- Fault-Tolerant Systems: The system should be designed to be fault-tolerant, meaning that it can continue to operate even if some of its components fail.\n",
    "- Backup Solutions: Regular backups of the system's data and configuration should be taken.\n",
    "- Recovery Plans: A detailed recovery plan should be in place to ensure that the system can be restored quickly in the event of a disaster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Scalability Patterns: Microservices, Event-Driven Architecture\n",
    "\n",
    "As a trading business grows, its trading platform needs to be able to scale to handle the increased load.\n",
    "\n",
    "- Microservices: Microservices is an architectural style that structures an application as a collection of small, independent services. This makes it easier to scale and maintain the application.\n",
    "- Event-Driven Architecture: In an event-driven architecture, the services communicate with each other by sending and receiving events. This allows for loose coupling between the services and makes it easier to build scalable and resilient systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21b6a900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RabbitMQ Event-Driven System (Conceptual) ---\n",
      "The code above shows a simple publisher and consumer for a RabbitMQ message queue.\n",
      "To run it, you would need to have a RabbitMQ server running.\n"
     ]
    }
   ],
   "source": [
    "# You would need to install pika first\n",
    "# pip install pika\n",
    "import pika\n",
    "\n",
    "def publisher():\n",
    "    \"\"\"A mock publisher that sends messages to a queue.\"\"\"\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    channel.queue_declare(queue='trades')\n",
    "    channel.basic_publish(exchange='', routing_key='trades', body='{\"symbol\": \"AAPL\", \"side\": \"buy\", \"quantity\": 100}')\n",
    "    print(\" [x] Sent 'trade'\")\n",
    "    connection.close()\n",
    "\n",
    "def consumer():\n",
    "    \"\"\"A mock consumer that receives messages from a queue.\"\"\"\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    channel.queue_declare(queue='trades')\n",
    "\n",
    "    def callback(ch, method, properties, body):\n",
    "        print(f\" [x] Received {body}\")\n",
    "\n",
    "    channel.basic_consume(queue='trades', on_message_callback=callback, auto_ack=True)\n",
    "    print(' [*] Waiting for messages. To exit press CTRL+C')\n",
    "    # channel.start_consuming() # This would block and wait for messages\n",
    "\n",
    "# To run this, you would need to have a RabbitMQ server running.\n",
    "# You would run the publisher and consumer in separate processes.\n",
    "# publisher()\n",
    "# consumer()\n",
    "\n",
    "print(\"--- RabbitMQ Event-Driven System (Conceptual) ---\")\n",
    "print(\"The code above shows a simple publisher and consumer for a RabbitMQ message queue.\")\n",
    "print(\"To run it, you would need to have a RabbitMQ server running.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Chapter 11 covers critical infrastructure and deployment strategies for robust, low-latency, secure, and scalable AI trading systems in production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mforge312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
