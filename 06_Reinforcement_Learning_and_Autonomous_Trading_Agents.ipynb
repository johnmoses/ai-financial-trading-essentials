{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Reinforcement Learning and Autonomous Trading Agents\n",
    "\n",
    "## 1. RL Fundamentals: MDPs, Value Functions, Policy Gradients\n",
    "\n",
    "Reinforcement Learning (RL) is a machine learning paradigm where an \"agent\" learns to make decisions by taking actions in an \"environment\" to maximize a cumulative \"reward\". This is often modeled as a Markov Decision Process (MDP), which provides a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker.\n",
    "\n",
    "- Value Functions: Estimate the expected cumulative reward from a given state, helping the agent understand how good a particular state is.\n",
    "- Policy Gradients: Directly optimize the agent's policy (what action to take in a given state) to maximize rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba11418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP components defined.\n"
     ]
    }
   ],
   "source": [
    "# Simple MDP representation for a trading problem\n",
    "# States: 0 for 'neutral', 1 for 'bullish', 2 for 'bearish' market sentiment\n",
    "states = [0, 1, 2]\n",
    "\n",
    "# Actions: 0 for 'hold', 1 for 'buy', 2 for 'sell'\n",
    "actions = [0, 1, 2]\n",
    "\n",
    "# Transition probabilities: P(s' | s, a)\n",
    "# A simplified, hypothetical transition model\n",
    "transition_probabilities = {\n",
    "    # If neutral (0)\n",
    "    (0, 'hold'): {0: 0.8, 1: 0.1, 2: 0.1},\n",
    "    (0, 'buy'): {0: 0.7, 1: 0.2, 2: 0.1},\n",
    "    (0, 'sell'): {0: 0.7, 1: 0.1, 2: 0.2},\n",
    "    # If bullish (1)\n",
    "    (1, 'hold'): {0: 0.1, 1: 0.8, 2: 0.1},\n",
    "    (1, 'buy'): {0: 0.1, 1: 0.9, 2: 0.0},\n",
    "    (1, 'sell'): {0: 0.2, 1: 0.7, 2: 0.1},\n",
    "    # If bearish (2)\n",
    "    (2, 'hold'): {0: 0.1, 1: 0.1, 2: 0.8},\n",
    "    (2, 'buy'): {0: 0.2, 1: 0.2, 2: 0.6},\n",
    "    (2, 'sell'): {0: 0.1, 1: 0.0, 2: 0.9},\n",
    "}\n",
    "\n",
    "# Rewards: R(s, a, s')\n",
    "# A simplified, hypothetical reward model\n",
    "rewards = {\n",
    "    # If bullish (1)\n",
    "    (1, 'buy'): 10,\n",
    "    (1, 'sell'): -10,\n",
    "    # If bearish (2)\n",
    "    (2, 'buy'): -10,\n",
    "    (2, 'sell'): 10,\n",
    "}\n",
    "\n",
    "print(\"MDP components defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Trading-Specific RL Algorithms: DQN, PPO, SAC for Financial Markets\n",
    "\n",
    "Not all RL algorithms are suitable for the complexities of financial markets. The notebook mentions three popular ones:\n",
    "\n",
    "- Deep Q-Networks (DQN): A value-based method that is effective in learning from historical data but can be unstable in volatile markets.\n",
    "- Proximal Policy Optimization (PPO): A policy-based method that offers more stable and reliable training by limiting the size of policy updates.\n",
    "- Soft Actor-Critic (SAC): An advanced algorithm that balances maximizing rewards with exploring new strategies, making it suitable for dynamic market conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5728d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table:\n",
      "[[0.36005604 0.46579227 0.13293887]\n",
      " [0.50529611 0.4328324  0.60762007]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simplified Q-learning for trading\n",
    "# Environment states: 0 for 'out of the market', 1 for 'in the market'\n",
    "# Actions: 0 for 'hold', 1 for 'buy', 2 for 'sell'\n",
    "q_table = np.zeros((2, 3))\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "episodes = 1000\n",
    "\n",
    "for episode in range(episodes):\n",
    "    # Simplified environment: random price changes\n",
    "    price_movement = np.random.choice([-1, 1])\n",
    "    \n",
    "    # For simplicity, we'll just toggle state\n",
    "    current_state = np.random.randint(0, 2)\n",
    "    \n",
    "    # Choose action (simplified exploration/exploitation)\n",
    "    if np.random.uniform(0, 1) < 0.5:\n",
    "        action = np.random.randint(0, 3)\n",
    "    else:\n",
    "        action = np.argmax(q_table[current_state, :])\n",
    "\n",
    "    # Simplified reward\n",
    "    reward = 0\n",
    "    if action == 1: # buy\n",
    "        reward = -price_movement\n",
    "    elif action == 2: # sell\n",
    "        reward = price_movement\n",
    "\n",
    "    # Q-learning formula\n",
    "    old_value = q_table[current_state, action]\n",
    "    next_state_max = np.max(q_table[1 - current_state, :])\n",
    "    \n",
    "    new_value = (1 - learning_rate) * old_value + learning_rate * (reward + discount_factor * next_state_max)\n",
    "    q_table[current_state, action] = new_value\n",
    "\n",
    "print(\"Q-table:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building Autonomous Trading Agents: State Design, Action Spaces, Reward Engineering\n",
    "\n",
    "Creating a successful trading agent requires careful design of its core components:\n",
    "\n",
    "- State Design: Defining what information the agent sees at each step. This could include market prices, technical indicators, and portfolio status.\n",
    "- Action Spaces: Defining the possible actions the agent can take, such as buying, selling, or holding an asset.\n",
    "- Reward Engineering: Crafting a reward function that aligns with the trading goal. A simple reward might be profit, while a more complex one could be a risk-adjusted return like the Sharpe ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: {'price': 100, 'position': 0, 'cash': 10000}\n",
      "State after buying: {'price': 102, 'position': 1, 'cash': 9900}, Reward: 0\n",
      "State after selling: {'price': 101, 'position': 0, 'cash': 10002}, Reward: 2\n"
     ]
    }
   ],
   "source": [
    "class TradingEnvironment:\n",
    "    def __init__(self, initial_cash, stock_price_history):\n",
    "        self.cash = initial_cash\n",
    "        self.stock_price_history = stock_price_history\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # shares held\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.cash = 10000\n",
    "        self.position = 0\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        return {'price': self.stock_price_history[self.current_step], 'position': self.position, 'cash': self.cash}\n",
    "\n",
    "    def step(self, action):\n",
    "        # actions: 0: hold, 1: buy, 2: sell\n",
    "        price = self.stock_price_history[self.current_step]\n",
    "        \n",
    "        if action == 1 and self.cash > price: # buy\n",
    "            self.position += 1\n",
    "            self.cash -= price\n",
    "        elif action == 2 and self.position > 0: # sell\n",
    "            self.position -= 1\n",
    "            self.cash += price\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.stock_price_history) - 1\n",
    "        \n",
    "        portfolio_value = self.cash + self.position * price\n",
    "        reward = portfolio_value - 10000 # reward is profit\n",
    "        \n",
    "        return self._get_state(), reward, done\n",
    "\n",
    "# Example usage\n",
    "price_history = [100, 102, 101, 103, 105]\n",
    "env = TradingEnvironment(initial_cash=10000, stock_price_history=price_history)\n",
    "state = env.reset()\n",
    "print(f\"Initial state: {state}\")\n",
    "\n",
    "# Simulate a few steps\n",
    "state, reward, done = env.step(1) # buy\n",
    "print(f\"State after buying: {state}, Reward: {reward}\")\n",
    "state, reward, done = env.step(2) # sell\n",
    "print(f\"State after selling: {state}, Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Agent Systems: Agent Orchestration and Coordination Strategies\n",
    "\n",
    "Instead of a single agent, a trading system can use multiple agents that may specialize in different assets, strategies, or market conditions. The challenge lies in orchestrating these agents to work together, avoid conflicting actions, and manage risk collectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f52148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final action from multi-agent system: buy\n"
     ]
    }
   ],
   "source": [
    "class Agent:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def decide(self, market_data):\n",
    "        # Each agent has its own logic\n",
    "        if self.name == \"MomentumTrader\" and market_data['momentum'] > 0.5:\n",
    "            return 'buy'\n",
    "        elif self.name == \"MeanReversionTrader\" and market_data['price'] < market_data['mean_price']:\n",
    "            return 'buy'\n",
    "        else:\n",
    "            return 'hold'\n",
    "\n",
    "# Orchestrator\n",
    "def run_multi_agent_system(market_data):\n",
    "    agents = [Agent(\"MomentumTrader\"), Agent(\"MeanReversionTrader\")]\n",
    "    decisions = {}\n",
    "    for agent in agents:\n",
    "        decisions[agent.name] = agent.decide(market_data)\n",
    "    \n",
    "    # A simple coordination strategy: take action if at least one agent wants to buy\n",
    "    if 'buy' in decisions.values():\n",
    "        final_action = 'buy'\n",
    "    else:\n",
    "        final_action = 'hold'\n",
    "        \n",
    "    return final_action\n",
    "\n",
    "# Example\n",
    "market_data = {'price': 100, 'mean_price': 105, 'momentum': 0.6}\n",
    "action = run_multi_agent_system(market_data)\n",
    "print(f\"Final action from multi-agent system: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RL in Portfolio Management: Dynamic Allocation and Rebalancing\n",
    "\n",
    "RL can be applied to the higher-level problem of portfolio management. An RL agent can learn to dynamically allocate capital across different assets and rebalance the portfolio over time to adapt to market changes and optimize for a specific objective, such as maximizing returns while minimizing risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24d5ee58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example portfolio action: [0.6, 0.2, 0.2]\n"
     ]
    }
   ],
   "source": [
    "# Conceptual example for portfolio management\n",
    "# State: includes market features and current portfolio weights\n",
    "state = {\n",
    "    'market_features': [0.1, 0.5, ...], # e.g., moving averages, volatility\n",
    "    'portfolio_weights': [0.5, 0.3, 0.2] # weights for assets A, B, C\n",
    "}\n",
    "\n",
    "# Action: new portfolio weights\n",
    "# The agent needs to learn a policy that maps state -> action\n",
    "action = [0.6, 0.2, 0.2] # new weights\n",
    "\n",
    "# Reward could be the change in portfolio value, adjusted for transaction costs\n",
    "def calculate_reward(old_weights, new_weights, prices_t, prices_t_plus_1):\n",
    "    # factor in transaction costs for rebalancing\n",
    "    transaction_costs = np.sum(np.abs(new_weights - old_weights)) * 0.001 # 0.1% cost\n",
    "    \n",
    "    # return of the portfolio\n",
    "    portfolio_return = np.sum(new_weights * (prices_t_plus_1 / prices_t - 1))\n",
    "    \n",
    "    return portfolio_return - transaction_costs\n",
    "\n",
    "print(f\"Example portfolio action: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sim-to-Real Transfer: From Backtesting to Live Trading\n",
    "\n",
    "An agent that performs well in a simulated environment (backtesting) may not perform well in live trading. This \"sim-to-real\" gap is a significant challenge. Bridging this gap involves:\n",
    "\n",
    "- Domain Adaptation: Adjusting the model to the nuances of the live market.\n",
    "- Risk Controls: Implementing safeguards to prevent catastrophic losses.\n",
    "- Continuous Retraining: Regularly updating the model with new market data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94c37a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiveTradingBot:\n",
    "    def __init__(self, model_path, api_key, api_secret):\n",
    "        # self.model = self.load_model(model_path)\n",
    "        # self.api = self.connect_to_exchange(api_key, api_secret)\n",
    "        self.risk_manager = self.setup_risk_management()\n",
    "        print(\"Live trading bot initialized.\")\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        # Load a pre-trained model from a file\n",
    "        print(f\"Loading model from {model_path}\")\n",
    "        # return loaded_model\n",
    "        pass\n",
    "\n",
    "    def connect_to_exchange(self, api_key, api_secret):\n",
    "        # Connect to a live exchange API\n",
    "        print(\"Connecting to exchange...\")\n",
    "        # return exchange_api_client\n",
    "        pass\n",
    "\n",
    "    def setup_risk_management(self):\n",
    "        # Initialize risk controls, e.g., max drawdown, position size limits\n",
    "        print(\"Risk management setup.\")\n",
    "        return {\"max_position_size\": 100}\n",
    "\n",
    "    def run(self):\n",
    "        # Main loop for live trading\n",
    "        # while True:\n",
    "            # 1. Get live market data\n",
    "            # live_data = self.api.get_market_data()\n",
    "            \n",
    "            # 2. Get decision from the RL model\n",
    "            # action = self.model.predict(live_data)\n",
    "            \n",
    "            # 3. Apply risk management rules\n",
    "            # if self.risk_manager.is_safe(action):\n",
    "                # 4. Execute trade\n",
    "                # self.api.execute_trade(action)\n",
    "            \n",
    "            # 5. Log and monitor\n",
    "            # self.log_activity()\n",
    "            \n",
    "            # time.sleep(60) # wait for the next candle\n",
    "        print(\"Running live trading loop (conceptual).\")\n",
    "\n",
    "# Conceptual usage\n",
    "# bot = LiveTradingBot(\"path/to/my/model.pkl\", \"YOUR_API_KEY\", \"YOUR_API_SECRET\")\n",
    "# bot.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. RL Performance Metrics and Evaluation Challenges\n",
    "\n",
    "Evaluating an RL trading agent goes beyond simple profit and loss. Key metrics include:\n",
    "\n",
    "Cumulative Return: The total return over a period.\n",
    "Maximum Drawdown: The largest peak-to-trough decline in portfolio value.\n",
    "Sharpe Ratio: A measure of risk-adjusted return.\n",
    "Policy Stability: How much the agent's strategy changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ff2c155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annualized Sharpe Ratio: 1.66\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_sharpe_ratio(returns, risk_free_rate=0.0):\n",
    "    \"\"\"\n",
    "    Calculates the Sharpe ratio of a series of returns.\n",
    "    \"\"\"\n",
    "    # Calculate excess returns\n",
    "    excess_returns = returns - risk_free_rate\n",
    "    \n",
    "    # Calculate mean and standard deviation of excess returns\n",
    "    mean_excess_return = np.mean(excess_returns)\n",
    "    std_dev_excess_return = np.std(excess_returns)\n",
    "    \n",
    "    # Calculate Sharpe ratio\n",
    "    if std_dev_excess_return == 0:\n",
    "        return 0\n",
    "    \n",
    "    sharpe_ratio = mean_excess_return / std_dev_excess_return\n",
    "    \n",
    "    # Annualize the Sharpe ratio (assuming daily returns)\n",
    "    annualized_sharpe_ratio = sharpe_ratio * np.sqrt(252)\n",
    "    \n",
    "    return annualized_sharpe_ratio\n",
    "\n",
    "# Example usage\n",
    "daily_returns = np.random.randn(252) * 0.01 # 252 trading days in a year\n",
    "sharpe = calculate_sharpe_ratio(daily_returns)\n",
    "print(f\"Annualized Sharpe Ratio: {sharpe:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Chapter 6 introduces core RL concepts, trading-specific algorithms, agent design considerations, multi-agent coordination, portfolio management, and challenges in live deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mforge312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
